# Reinforcement-Learning-
learn Reinforcement Learning notes recently 
强化学习

[TOC]

# 一、简介

​	强化学习会在没有任何标签的情况下，通过先尝试做出一些行为得到一个结果，通过这个结果是对还是错的反馈，调整之前的行为，就这样不断的调整，算法能够学习到在什么样的情况下选择什么样的行为可以得到最好的结果。

强化学习的结果反馈有延时，有时候可能需要走了很多步以后才知道以前的某一步的选择是好还是坏，而监督学习做了比较坏的选择会立刻反馈给算法。

而且强化学习面对的输入总是在变化，每当算法做出一个行为，它影响下一次决策的输入，而监督学习的输入是独立同分布的。





现有强化学习方法: Q-learning、Policy gradient、DQN、DDPG



通过不断的学习获取标签，然后学习哪些数据对应哪些标签，通过学习规律尽可能获得高分。



| 通过价值选行为 | 直接选行为      | 想象环境并从中学习 |
| -------------- | --------------- | ------------------ |
| Q learning     | Policy Gradient | Model based RL     |
| Sarsa          |                 |                    |
| Deep Q Network |                 |                    |



## “强化学习之父”

加拿大埃德蒙顿的阿尔伯塔大学（UAlberta）可谓是强化学习重镇，这项技术的缔造者之一萨顿（Rich Sutton）在这里任教。

他对强化学习的重要贡献包括 *时序差分学习*  和 *策略梯度方法*。



Q： 强化学习的主要思想是什么？它与监督学习有何不同？

萨顿： 在与世界的正常互动过程中， 强化学习会通过试错法利用奖励来学习 。因此，它跟自然学习过程非常相似，而与监督学习不同。



例如，语音识别目前通过监督学习来完成，需要使用大量的语音数据集和正确的文本内容。这些文本内容就是一种监督信号，等系统开始工作、输入了新的语音时，就没有这个监督信号了。

而AI打游戏，通常就是通过强化学习来实现的，需要利用游戏的结果作为奖励。即使你玩了一个新游戏，也会看到自己是赢是输，并且可以用强化学习算法来提高你的游戏技术。

监督式游戏学习方法则需要借助一些“正确”的动作来实现，这些动作可以来自人类专家。这很方便，但在正常的游戏中是不可用的，而且会导致学习系统的技能局限在人类专家的技能范畴内。在强化学习中，你可以用较少的训练信息，这样做的优势是信息更充足，而且不受监督者的技能限制。



Q： 什么是深度强化学习？它与强化学习有何不同?

萨顿： 深度强化学习是深度学习和强化学习的结合。这两种学习方式在很大程度上是正交问题，二者结合得很好。

简而言之，强化学习需要通过数据逼近函数的方法来部署其所有的组件——值函数、策略、世界模型、状态更新——而深度学习是最近开发的函数逼近器中最新、最成功一个。

我们的教科书主要介绍线性函数逼近器，并给出一般情况下的方程。我们在应用一章和一节中介绍了神经网络，但要充分了解深度强化学习，就必须用Goodfellow、Bengio、和Courville的《深度学习》来补充我们的书。



## 新思路

1. 打游戏通过利用游戏的结果作为奖励，那预测和分类可以通过预测的RMSE和R方or分类准确率作为奖励。来运用强化学习解决预测问题。

2. 强化学习是用来求解mdp，而某些预测问题可以建模成mdp。

   我的回答是：是可以用来解决预测问题，但不是全部

3. 强化学习更适合用来决策而不是预测，当然不排除特殊情况。每一种方法都有相应适合的场景，还是要针对具体的场景。



# 二、强化学习方法汇总

## 不理解环境 vs 理解环境 

不理解环境Model-Free RL：不用理解环境，环境给了什么就是什么。

理解环境Model-Based RL：用模型来表示环境，理解了环境，就用模型来代替环境。了解所处环境，想象一个虚拟环境进行学习。(只是比Model-Free 多了想象环境，方法还是一样)能够想象所有情况，并选择最好的那种，并根据最好的情况来采取下一步的策略。

| 不理解环境(Model-Free RL)          | 理解环境(Model-Based RL) |
| ---------------------------------- | ------------------------ |
| Q learning、Sarsa、Policy Gradient |                          |





## 概率 vs 价值 

基于概率是强化学习中最直接的一种, 他能通过感官分析所处的环境, 直接输出下一步要采取的各种动作的概率, 然后根据概率采取行动, 所以每种动作都有可能被选中, 只是可能性不同.

 而基于价值的方法输出则是所有动作的价值, 我们会根据最高价值来选着动作, 相比基于概率的方法, 基于价值的决策部分更为铁定, 毫不留情, 就选价值最高的, 而基于概率的, 即使某个动作的概率最高, 但是还是不一定会选到他.

| 基于概率(Policy-Based RL)                                    | 基于价值(Valued-Based RL)                                    |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [Policy Gradients](https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/4-07-PG/) | [Q learning](https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/4-03-q-learning/), [Sarsa](https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/4-04-sarsa/) |

而且我们还能结合这两类方法的优势之处, 创造更牛逼的一种方法, 叫做 [Actor-Critic](https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/4-08-AC/), actor 会基于概率做出动作, 而 critic 会对做出的动作给出动作的价值, 这样就在原有的 policy gradients 上加速了学习过程.概率 vs 价值 





## 回合更新 vs 单步更新 

回合更新：指的是游戏开始后, 我们要等待游戏结束, 然后再总结这一回合中的所有转折点, 再更新我们的行为准则。

单步更新：是在游戏进行中每一步都在更新, 不用等待游戏的结束, 这样我们就能边玩边学习了。

| 回合更新                  | 单步更新(Temporal-Difference update)-效率高 |
| ------------------------- | ------------------------------------------- |
| Monte-Carlo learning      | Qlearning, Sarsa                            |
| 基础版的 policy gradients | 升级版的 policy gradients                   |

因为单步更新更有效率, 所以现在大多方法都是基于单步更新. 比如有的强化学习问题并不属于回合问题.



## 在线学习vs 离线学习

在线学习：指我必须本人在场, 并且一定是本人边玩边学习；

离线学习：是你可以选择自己玩, 也可以选择看着别人玩, 通过看别人玩来学习别人的行为准则, 离线学习 同样是从过往的经验中学习, 但是这些过往的经历没必要是自己的经历, 任何人的经历都能被学习. 或者我也不必要边玩边学习, 我可以白天先存储下来玩耍时的记忆, 然后晚上通过离线学习来学习白天的记忆.

| 在线学习(On-Policy) | 离线学习(Off-Policy) |
| ------------------- | -------------------- |
| Sarsa               | Q learning           |
| Sarsa lambda        | Deep-Q-Network       |







# 三、强化学习的应用

- 经典应用

由于强化学习做的是序列的预测和序列的学习，所以它以往主要的一个应用领域，是做机器控制，比如说直升机的操控。

![南京大学俞扬博士万字演讲全文：强化学习前沿（上）](https://static.leiphone.com/uploads/new/article/740_740/201703/58ca4976212a9.png?imageMogr2/format/jpg/quality/90)

在直升机的应用里面，智能体就是直升机，环境是其飞行空域，状态可以是直升机的高度、速度、姿态等等，采取的决策是操纵杆指令。我们希望直升机能够做出我们想要的轨迹，但是又不会掉下来。这些目标可以作为直升机的奖赏，让它来学习一个策略，以实时控制直升机的运动。

- **更多的应用**

有不少真实世界的应用，其背后面临的问题都符合强化学习的问题设定。比如说股市预测和商品推荐。

![南京大学俞扬博士万字演讲全文：强化学习前沿（上）](https://static.leiphone.com/uploads/new/article/740_740/201703/58ca499d031ac.png?imageMogr2/format/jpg/quality/90)

**1、股市预测**

首先这是一个序列决策，要做出很多的决策，每做一个决策动作都要看当前的股市的状态如何，动作可以是买、卖，和观望。

那为什么这个问题是强化学习问题呢？也有很多序列决策有可能并不是强化学习的问题，我们靠什么判断序列决策到底是不是强化学习呢？关键因素在于：决策放到环境里面执行以后，是否会改变这个环境。

在股市交易时，成交的那一刻会决定股价是多少，这相当于决策改变了环境。有时可能很少的交易，也会引起其他投资人对股市的预期，从而影响股市的走势。

 **2、另一个例子是商品推荐**

为什么推荐问题也是可以看作它是一个强化学习问题呢？推荐系统会在网页上放置推荐展品，而用户的购买行为和推荐行为是有关系的。对于推荐的展品，即使比较普通也可以收到很多客户浏览，而优秀的商品如果没有被推荐出来则可能无人问津。总的来说，决策会影响整个系统。

**3、近期的应用**

在处理结构化数据时，比如做自然语言处理、把离散结构的知识库用到学习系统，会面临一个问题，即我们面对的语言或者知识库难以融入可微分模型中。一些研究者最近就想出来一些办法，把一个句子输出的词或知识库里面的操作，作为强化学习的动作，这样通过强化学习一些方法的可微分性纳入整个可微分学习系统中来。按照深度学习中比较流行的端到端训练的说法，强化学习的框架纳入进来以后，可把整个系统变成端到端的学习。





### 四、强化学习算法

## 1.Q Learning

Q learning 是一种 off-policy 离线学习法, 它能学习当前经历着的, 也能学习过去经历过的, 甚至是学习别人的经历. 





## 2.Sarsa







## 3.Sarsa(lambda)









## 4.DQN

两大因素支撑着 DQN 使得它变得无比强大. 这两大因素就是 Experience replay 和 Fixed Q-targets.
